{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'item_id': '123373', 'waist': '29', 'size': 7, 'quality': 5, 'cup size': 'd', 'hips': '38', 'bra size': '34', 'category': 'new', 'bust': '36', 'height': '5ft 6in', 'user_name': 'Emily', 'length': 'just right', 'fit': 'small', 'user_id': '991571'}, {'item_id': '123373', 'waist': '31', 'size': 13, 'quality': 3, 'cup size': 'b', 'hips': '30', 'bra size': '36', 'category': 'new', 'length': 'just right', 'height': '5ft 2in', 'user_name': 'sydneybraden2001', 'fit': 'small', 'user_id': '587883'}, {'item_id': '123373', 'waist': '30', 'size': 7, 'quality': 2, 'cup size': 'b', 'shoe size': '9.00', 'bra size': '32', 'category': 'new', 'length': 'slightly long', 'height': '5ft 7in', 'user_name': 'Ugggh', 'fit': 'small', 'user_id': '395665'}, {'item_id': '123373', 'category': 'new', 'size': 21, 'quality': 5, 'user_name': 'alexmeyer626', 'length': 'just right', 'fit': 'fit', 'cup size': 'dd/e', 'user_id': '875643'}, {'item_id': '123373', 'category': 'new', 'size': 18, 'quality': 5, 'user_name': 'dberrones1', 'length': 'slightly long', 'fit': 'small', 'bra size': '36', 'cup size': 'b', 'user_id': '944840', 'height': '5ft 2in'}, {'item_id': '123373', 'waist': '27', 'size': 11, 'quality': 5, 'cup size': 'c', 'hips': '41', 'bra size': '36', 'category': 'new', 'length': 'just right', 'height': '5ft 4in', 'user_name': 'Doreenajane', 'fit': 'small', 'user_id': '162012'}, {'item_id': '123373', 'waist': '26', 'size': 5, 'quality': 1, 'cup size': 'b', 'bra size': '32', 'category': 'new', 'length': 'just right', 'height': '5ft 3in', 'user_name': 'barbiejenks', 'fit': 'large', 'user_id': '114843'}, {'item_id': '123373', 'shoe size': '8.50', 'size': 11, 'quality': 5, 'cup size': 'd', 'hips': '42.0', 'bra size': '38', 'category': 'new', 'length': 'just right', 'height': '5ft 5in', 'user_name': 'brettloie', 'fit': 'small', 'user_id': '58869'}, {'item_id': '123373', 'shoe size': '11.00', 'size': 30, 'quality': 4, 'shoe width': 'wide', 'cup size': 'd', 'hips': '50.0', 'bra size': '42', 'category': 'new', 'length': 'just right', 'height': '5ft 10in', 'user_name': 'francescaviola', 'fit': 'small', 'user_id': '279568'}, {'item_id': '123373', 'shoe size': '9.00', 'size': 13, 'quality': 5, 'cup size': 'dd/e', 'hips': '41.0', 'bra size': '36', 'category': 'new', 'bust': '39', 'height': '5ft 6in', 'user_name': 'laurenpolzin', 'length': 'just right', 'fit': 'fit', 'user_id': '950172'}]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import copy\n",
    "import gzip\n",
    "import json\n",
    "import string\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "def feature_label_default(data):\n",
    "    return \"fit\"\n",
    "\n",
    "def feature_label(data):\n",
    "    return data[\"fit\"]\n",
    "def feature_X(data,encoding):\n",
    "    hotencoding=encoding[catID[data['category']]]\n",
    "    return [1,data['height'],data['weight'],int(data['rating']),int(data['size'])]+hotencoding\n",
    "def str_to_inch(s):\n",
    "    s = s.replace('\\\"', '')\n",
    "    height=s.split(\"\\'\")\n",
    "    return int(height[0])*12+int(height[1])\n",
    "def str_to_weight(s):\n",
    "    s = s.replace('lbs', '')\n",
    "    return int(s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = list(parseData(\"modcloth_final_data.json\"))\n",
    "#random.shuffle(data)\n",
    "print(test_data[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'dress': 72894, 'down': 387, 'gown': 34445, 'sheath': 15189, 'top': 3923, 'shift': 4136, 'maxi': 2678, 'skirt': 1241, 'jumpsuit': 4159, 'sweater': 948, 'romper': 2517, 'jacket': 1909, 'mini': 1438, 'blazer': 633, 'cardigan': 203, 'pant': 85, 'blouson': 11, 'coat': 792, 'tank': 145, 'blouse': 504, 'shirtdress': 583, 'ballgown': 13, 'hoodie': 11, 'kimono': 24, 'pullover': 46, 'culottes': 163, 'vest': 222, 'turtleneck': 23, 'shirt': 220, 'pants': 340, 'frock': 170, 'trouser': 50, 'tunic': 137, 'suit': 108, 'parka': 13, 'bomber': 103, 'legging': 81, 'sweatshirt': 104, 'midi': 46, 'jeans': 4, 'duster': 12, 'poncho': 34, 't-shirt': 11, 'leggings': 95, 'peacoat': 33, 'print': 90, 'cami': 12, 'knit': 42, 'culotte': 60, 'overalls': 6, 'sweatershirt': 3, 'cape': 69, 'tee': 20, 'trousers': 12, 'caftan': 4, 'skort': 4, 'for': 5, 'jogger': 4, 'skirts': 5, 'trench': 20, 'kaftan': 13, 'combo': 8, 'henley': 7, 'tight': 13, 'sweatpants': 1, 'crewneck': 1, 'buttondown': 1, 'overcoat': 1})\n"
     ]
    }
   ],
   "source": [
    "#SET UP CATEGORY FOR data\n",
    "categoryCounts = defaultdict(int)\n",
    "for d in data1:\n",
    "    categoryCounts[d['category']] += 1\n",
    "categories = [c for c in categoryCounts]\n",
    "catID = dict(zip(list(categories),range(len(categories))))\n",
    "print(categoryCounts)\n",
    "\n",
    "hot_encoding={}                                        #hot encoding of 0-41\n",
    "for i in range(len(categories)):\n",
    "    encoding=[0]*len(categories)\n",
    "    encoding[i]=1\n",
    "    hot_encoding[i]=encoding\n",
    "    \n",
    "#print(hot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TruePos(pred,y,label):\n",
    "    res=0\n",
    "    for pred_, y_ in zip(pred, y):\n",
    "        if pred_ == y_ and y_== label:\n",
    "            \n",
    "            res+=1\n",
    "    return res\n",
    "\n",
    "def FalsePos(pred,y,label):\n",
    "    res=0\n",
    "    for pred_, y_ in zip(pred, y):\n",
    "        if pred_ == label and  y_ != label:\n",
    "            \n",
    "            res+=1\n",
    "    return res\n",
    "\n",
    "def TrueNeg(pred,y,label):\n",
    "    res=0\n",
    "    for pred_, y_ in zip(pred, y):\n",
    "        if  pred_ != label and label != y_:\n",
    "            \n",
    "                res+=1\n",
    "    return res\n",
    "\n",
    "def FalseNeg(pred,y,label):\n",
    "    res=0\n",
    "    for pred_, y_ in zip(pred, y):\n",
    "        if label!= pred_ and y_==label:\n",
    "            \n",
    "            res+=1\n",
    "    return res\n",
    "def BER(pred,train_y,label):\n",
    "    TP=TruePos(pred,train_y,label)\n",
    "    FP=FalsePos(pred,train_y,label)\n",
    "    TN=TrueNeg(pred,train_y,label)\n",
    "    FN=FalseNeg(pred,train_y,label)\n",
    "    TPR= TP/(TP+FN)\n",
    "    TNR= TN/(TN+FP)\n",
    "    FPR= FP/(FP+TN)\n",
    "    FNR=FN/(FN+TP)\n",
    "    if (TP+FP)==0:\n",
    "        PERCISION=0\n",
    "    else:\n",
    "        PERCISION= TP/(TP+FP)\n",
    "    \n",
    "    #BER= 1-0.5*(TPR+TNR)\n",
    "    BER= 0.5*(FPR+FNR)\n",
    "    print(\"Number of time predicted\",label,\" correct :\", TP)\n",
    "    print(\"Number of time predicted\",label,\" Wrong :\", FP)\n",
    "    print(\"Number of time predicted different than actual\",label,\"  :\", FN)\n",
    "    \n",
    "    print(\"RECALL \",label,\" is :\", TPR)\n",
    "    print(\"TNR \",label,\" is :\", TNR)\n",
    "    print(\"PERCISION \",label,\" is :\", PERCISION)\n",
    "    print(\"FPR \",label,\" is :\", FPR)\n",
    "    print(\"BER \",label,\" is :\", BER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('renttherunway_final_data.json', \"r\")\n",
    "data = list([json.loads(line) for line in open('renttherunway_final_data.json', 'r')])\n",
    "    \n",
    "data1=[]\n",
    "data_test=[]\n",
    "for i in data:\n",
    "    if 'weight' in i and 'bust size' in i and 'height' in i and 'rating' in i and 'review_text' in i:\n",
    "        if i['rating'] != None:\n",
    "            i['weight']=str_to_weight(i['weight'])\n",
    "            i['height']=str_to_inch(i['height'])\n",
    "            data1.append(i)\n",
    "\n",
    "for i in test_data:\n",
    "    if 'weight' in i and 'bust size' in i and 'height' in i and 'rating' in i and 'review_text' in i:\n",
    "        i['weight']=str_to_weight(i['weight'])\n",
    "        i['height']=str_to_inch(i['height'])\n",
    "        data_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 82790\n",
      "136152 15132 151284\n"
     ]
    }
   ],
   "source": [
    "#print(len(data_train),len(data))\n",
    "print(len(data_test),len(test_data))    #different data sets cannot be test with training set\n",
    "\n",
    "random.shuffle(data1)\n",
    "data_train=data1[:len(data1)//10*9]\n",
    "data_test=data1[len(data1)//10*9:]\n",
    "print(len(data_train),len(data_test),len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "fit : fit\n",
      "user_id : 420272\n",
      "bust size : 34d\n",
      "item_id : 2260466\n",
      "weight : 137\n",
      "rating : 10\n",
      "rented for : vacation\n",
      "review_text : An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\n",
      "body type : hourglass\n",
      "review_summary : So many compliments!\n",
      "category : romper\n",
      "height : 68\n",
      "size : 14\n",
      "age : 28\n",
      "review_date : April 20, 2016\n"
     ]
    }
   ],
   "source": [
    "print(type(data[0]))\n",
    "for key in data[0]:\n",
    "    print(key,':',data[0][key])\n",
    "\n",
    "\n",
    "\n",
    "string1= \"5'8\\\"\"\n",
    "#print(data_train[1]['height'])\n",
    "#print(str_to_inch(data_train[1]['height']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "#print(test_data[:1])\n",
    "#l=[feature_height(d) for d in data_train]\n",
    "dic= defaultdict(int)\n",
    "#print(l[0])\n",
    "#for i in l:                                       #num of fit/small/large\n",
    "#    j=str(i)\n",
    "#    dic[j]+=1\n",
    "\n",
    "p=[d['category'] for d in data_train]             #num of items per category\n",
    "#for i in p:\n",
    "#    j=str(i)\n",
    "#    dic[j]+=1\n",
    "    \n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible missing values:\n",
    "\n",
    "#bust size\n",
    "#weight\n",
    "#rating\n",
    "#review_text\n",
    "#height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPeritem = defaultdict(set)\n",
    "itemPerUser = defaultdict(set)\n",
    "for d in data:\n",
    "    user,item = d['user_id'], d['item_id']\n",
    "    usersPeritem[item].add(user)\n",
    "    itemPerUser[user].add(item)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in itemPerUser:\n",
    "   # print(len(itemPerUser[i]))\n",
    "#print(len(itemPerUser))          #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[feature_X(d,hot_encoding) for d in data_train]\n",
    "X_test=[feature_X(d,hot_encoding) for d in data_test]\n",
    "\n",
    "Y_train=[feature_label(d) for d in data_train]\n",
    "Y_test=[feature_label(d) for d in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15132 15132\n",
      "accuracy feature:weight,height,rating 0.732355273592387\n",
      "done..\n",
      "Number of time predicted fit  correct : 11082\n",
      "Number of time predicted fit  Wrong : 4050\n",
      "Number of time predicted different than actual fit   : 0\n",
      "RECALL  fit  is : 1.0\n",
      "TNR  fit  is : 0.0\n",
      "PERCISION  fit  is : 0.732355273592387\n",
      "FPR  fit  is : 1.0\n",
      "BER  fit  is : 0.5\n",
      "Number of time predicted large  correct : 0\n",
      "Number of time predicted large  Wrong : 0\n",
      "Number of time predicted different than actual large   : 1972\n",
      "RECALL  large  is : 0.0\n",
      "TNR  large  is : 1.0\n",
      "PERCISION  large  is : 0\n",
      "FPR  large  is : 0.0\n",
      "BER  large  is : 0.5\n",
      "Number of time predicted small  correct : 0\n",
      "Number of time predicted small  Wrong : 0\n",
      "Number of time predicted different than actual small   : 2078\n",
      "RECALL  small  is : 0.0\n",
      "TNR  small  is : 1.0\n",
      "PERCISION  small  is : 0\n",
      "FPR  small  is : 0.0\n",
      "BER  small  is : 0.5\n"
     ]
    }
   ],
   "source": [
    "baseline=numpy.array([feature_label_default(d) for d in data_test])\n",
    "print(len(baseline),len(Y_test))\n",
    "correct_base = baseline==Y_test\n",
    "\n",
    "print(\"accuracy feature:weight,height,rating\", sum(correct_base)/len(correct_base))\n",
    "#print(len(correct1))\n",
    "print('done..')\n",
    "\n",
    "BER(baseline,Y_test,'fit')\n",
    "BER(baseline,Y_test,'large')\n",
    "BER(baseline,Y_test,'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=2.0,)\n",
    "mod.fit(X_train, Y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:weight,height,rating 0.7340074015331747\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod.predict(X_test)\n",
    "correct1 = pred_valid==Y_test\n",
    "\n",
    "print(\"accuracy feature:weight,height,rating\", sum(correct1)/len(correct1))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:weight,height,rating 0.7340074015331747\n",
      "Number of time predicted fit  correct : 10972\n",
      "Number of time predicted fit  Wrong : 3805\n",
      "Number of time predicted different than actual fit   : 110\n",
      "RECALL  fit  is : 0.9900739938639235\n",
      "TNR  fit  is : 0.06049382716049383\n",
      "PERCISION  fit  is : 0.7425052446369358\n",
      "FPR  fit  is : 0.9395061728395062\n",
      "BER  fit  is : 0.47471608948779137\n",
      "Number of time predicted large  correct : 35\n",
      "Number of time predicted large  Wrong : 76\n",
      "Number of time predicted different than actual large   : 1937\n",
      "RECALL  large  is : 0.017748478701825558\n",
      "TNR  large  is : 0.994224924012158\n",
      "PERCISION  large  is : 0.3153153153153153\n",
      "FPR  large  is : 0.005775075987841946\n",
      "BER  large  is : 0.4940132986430082\n",
      "Number of time predicted small  correct : 100\n",
      "Number of time predicted small  Wrong : 144\n",
      "Number of time predicted different than actual small   : 1978\n",
      "RECALL  small  is : 0.04812319538017324\n",
      "TNR  small  is : 0.9889688984219397\n",
      "PERCISION  small  is : 0.4098360655737705\n",
      "FPR  small  is : 0.011031101578060365\n",
      "BER  small  is : 0.48145395309894357\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy feature:weight,height,rating\", sum(correct1)/len(correct1))\n",
    "BER(pred_valid,Y_test,'fit')\n",
    "BER(pred_valid,Y_test,'large')\n",
    "BER(pred_valid,Y_test,'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'fit': 14777, 'large': 111, 'small': 244}) defaultdict(<class 'int'>, {'large': 1972, 'fit': 11082, 'small': 2078})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dic1= defaultdict(int)\n",
    "dic2=defaultdict(int)\n",
    "\n",
    "#print(l[0])\n",
    "for i in pred_valid:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic1[j]+=1\n",
    "for i in Y_test:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic2[j]+=1\n",
    "    \n",
    "print(dic1,dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Y_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit', 'fit', 'fit', 'fit', 'fit']\n"
     ]
    }
   ],
   "source": [
    "print([feature_label(d) for d in data_train[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try bag of words methods\n",
    "sp = set(string.punctuation)\n",
    "wordCount = defaultdict(int)\n",
    "for d in data1:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    for w in tokens:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44034\n"
     ]
    }
   ],
   "source": [
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done..\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "word_num = len(counts)//10\n",
    "words = [x[1] for x in counts[:word_num]]\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "filterword = [w for w in words if not w in stopwords]\n",
    "#filterword = [w for w in words]\n",
    "wordId = dict(zip(filterword, range(len(filterword))))\n",
    "wordSet = set(filterword)\n",
    "word_df = dict(zip(filterword, [0]*(len(filterword))))\n",
    "\n",
    "for d in data1:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    sets= set(tokens)\n",
    "    for i in sets:\n",
    "        if i in filterword:\n",
    "            word_df[i]+=1\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "word_df1= copy.deepcopy(word_df)\n",
    "N=len(data1)\n",
    "for i in word_df1:\n",
    "    word_df1[i]= math.log(N/word_df1[i])\n",
    "#print(word_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done..\n"
     ]
    }
   ],
   "source": [
    "def feature1(datum):\n",
    "    feat = [0]*len(filterword)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    for w in tokens:\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    sets=set(tokens)\n",
    "    for s in sets:\n",
    "        if s in wordSet:\n",
    "            feat[wordId[s]]=feat[wordId[s]]*word_df1[s]\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "nf1 = len(feature1(data1[0]))\n",
    "\n",
    "X1 = lil_matrix((len(data1), nf1))\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    \n",
    "    x = feature1(data1[i])\n",
    "    for j in range(nf1):\n",
    "        if x[j]:\n",
    "            X1[i,j] = x[j]\n",
    "\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain1 = X1[:len(data1)//10*9]\n",
    "\n",
    "Xtest1 = X1[len(data1)//10*9:]\n",
    "\n",
    "\n",
    "\n",
    "mod_ifdif = linear_model.LogisticRegression(C=2.0,random_state=0)\n",
    "mod_ifdif.fit(Xtrain1, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words idf 0.7849590272270685\n",
      "done..\n",
      "done\n",
      "Number of time predicted fit  correct : 10391\n",
      "Number of time predicted fit  Wrong : 2379\n",
      "Number of time predicted different than actual fit   : 691\n",
      "RECALL  fit  is : 0.9376466341815557\n",
      "TNR  fit  is : 0.41259259259259257\n",
      "PERCISION  fit  is : 0.8137039937353171\n",
      "FPR  fit  is : 0.5874074074074074\n",
      "BER  fit  is : 0.32488038661292584\n",
      "Number of time predicted large  correct : 749\n",
      "Number of time predicted large  Wrong : 425\n",
      "Number of time predicted different than actual large   : 1223\n",
      "RECALL  large  is : 0.37981744421906694\n",
      "TNR  large  is : 0.9677051671732523\n",
      "PERCISION  large  is : 0.6379897785349233\n",
      "FPR  large  is : 0.03229483282674772\n",
      "BER  large  is : 0.3262386943038404\n",
      "Number of time predicted small  correct : 738\n",
      "Number of time predicted small  Wrong : 450\n",
      "Number of time predicted different than actual small   : 1340\n",
      "RECALL  small  is : 0.3551491819056785\n",
      "TNR  small  is : 0.9655278075685614\n",
      "PERCISION  small  is : 0.6212121212121212\n",
      "FPR  small  is : 0.03447219243143864\n",
      "BER  small  is : 0.33966150526288\n"
     ]
    }
   ],
   "source": [
    "pred_valid2= mod_ifdif.predict(Xtest1)\n",
    "correct2 = pred_valid2==Y_test\n",
    "print(\"accuracy feature:bag of words idf\", sum(correct2)/len(correct2))\n",
    "\n",
    "\n",
    "#print(len(correct1))\n",
    "print('done..')\n",
    "print('done')\n",
    "\n",
    "BER(pred_valid2,Y_test,'fit')\n",
    "BER(pred_valid2,Y_test,'large')\n",
    "BER(pred_valid2,Y_test,'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'fit': 12782, 'large': 1222, 'small': 1128}) defaultdict(<class 'int'>, {'large': 1972, 'fit': 11082, 'small': 2078})\n"
     ]
    }
   ],
   "source": [
    "dic1= defaultdict(int)\n",
    "dic2=defaultdict(int)\n",
    "\n",
    "#print(l[0])\n",
    "for i in pred_valid1:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic1[j]+=1\n",
    "for i in Y_test:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic2[j]+=1\n",
    "    \n",
    "print(dic1,dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'predict fit but wrong': 3425, 'fit': 9357, 'predicted large but wrong': 1060, 'predict small but wrong': 988, 'large': 162, 'small': 140})\n"
     ]
    }
   ],
   "source": [
    "##i fdif result\n",
    "dic1= defaultdict(int)\n",
    "dic2=defaultdict(int)\n",
    "dic3=defaultdict(int)\n",
    "for i,j in zip(pred_valid1,Y_test):\n",
    "    if(i==j and i =='small'):\n",
    "        dic1[i]+=1\n",
    "    elif (i==j and i =='large'):\n",
    "        dic1[i]+=1\n",
    "    elif(i==j and i =='fit'):\n",
    "        dic1[i]+=1\n",
    "    elif(i=='small'):\n",
    "        dic1['predict small but wrong']+=1\n",
    "    elif(i=='large'):\n",
    "        dic1['predicted large but wrong']+=1\n",
    "    else:\n",
    "        dic1['predict fit but wrong']+=1\n",
    "print(dic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done..\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "    hotencoding=hot_encoding[catID[datum['category']]]\n",
    "    feat = [0]*len(filterword)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in sp])\n",
    "    tokens = r.split()\n",
    "    for w in tokens:\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    #feat.append(datum['height'])\n",
    "    #feat.append(datum['weight'])\n",
    "    \n",
    "    feat.append(datum['rating'])\n",
    "    feat= feat+hotencoding\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "nf = len(feature(data1[0]))\n",
    "\n",
    "X = lil_matrix((len(data1), nf))\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    \n",
    "    x = feature(data1[i])\n",
    "    for j in range(nf):\n",
    "        if x[j]:\n",
    "            X[i,j] = x[j]\n",
    "\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(X!=X1):\n",
    "#    print(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = X[:len(data1)//10*9]\n",
    "\n",
    "Xtest = X[len(data1)//10*9:]\n",
    "\n",
    "\n",
    "\n",
    "mod1 = linear_model.LogisticRegression(C=2.0)\n",
    "mod1.fit(Xtrain, Y_train)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "Number of time predicted fit  correct : 10545\n",
      "Number of time predicted fit  Wrong : 2315\n",
      "Number of time predicted different than actual fit   : 537\n",
      "RECALL  fit  is : 0.9515430427720628\n",
      "TNR  fit  is : 0.4283950617283951\n",
      "PERCISION  fit  is : 0.8199844479004665\n",
      "FPR  fit  is : 0.571604938271605\n",
      "BER  fit  is : 0.3100309477497711\n",
      "Number of time predicted large  correct : 804\n",
      "Number of time predicted large  Wrong : 321\n",
      "Number of time predicted different than actual large   : 1168\n",
      "RECALL  large  is : 0.4077079107505071\n",
      "TNR  large  is : 0.9756079027355623\n",
      "PERCISION  large  is : 0.7146666666666667\n",
      "FPR  large  is : 0.02439209726443769\n",
      "BER  large  is : 0.3083420932569653\n",
      "Number of time predicted small  correct : 784\n",
      "Number of time predicted small  Wrong : 363\n",
      "Number of time predicted different than actual small   : 1294\n",
      "RECALL  small  is : 0.37728585178055823\n",
      "TNR  small  is : 0.9721924314386395\n",
      "PERCISION  small  is : 0.6835222319093287\n",
      "FPR  small  is : 0.0278075685613605\n",
      "BER  small  is : 0.32526085839040114\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid3= mod1.predict(Xtest)\n",
    "correct3 = pred_valid3==Y_test\n",
    "#BER(pred_valid,Y_test)\n",
    "print(\"accuracy feature:bag of words\", sum(correct3)/len(correct3))\n",
    "BER(pred_valid3,Y_test,'fit')\n",
    "BER(pred_valid3,Y_test,'large')\n",
    "BER(pred_valid3,Y_test,'small')\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP=FalsePos(pred_valid,Y_test)\n",
    "#print(pred_valid,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  0.01 0.7107454401268835\n",
      "Number of time predicted fit  correct : 8066\n",
      "Number of time predicted fit  Wrong : 931\n",
      "Number of time predicted different than actual fit   : 3016\n",
      "RECALL  fit  is : 0.7278469590326656\n",
      "TNR  fit  is : 0.7701234567901235\n",
      "PERCISION  fit  is : 0.8965210625764144\n",
      "FPR  fit  is : 0.22987654320987655\n",
      "BER  fit  is : 0.25101479208860544\n",
      "Number of time predicted large  correct : 1288\n",
      "Number of time predicted large  Wrong : 1645\n",
      "Number of time predicted different than actual large   : 684\n",
      "RECALL  large  is : 0.6531440162271805\n",
      "TNR  large  is : 0.875\n",
      "PERCISION  large  is : 0.43914081145584727\n",
      "FPR  large  is : 0.125\n",
      "BER  large  is : 0.23592799188640973\n",
      "Number of time predicted small  correct : 1401\n",
      "Number of time predicted small  Wrong : 1801\n",
      "Number of time predicted different than actual small   : 677\n",
      "RECALL  small  is : 0.6742059672762272\n",
      "TNR  small  is : 0.8620346254021756\n",
      "PERCISION  small  is : 0.4375390381011868\n",
      "FPR  small  is : 0.13796537459782443\n",
      "BER  small  is : 0.23187970366079866\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  0.1 0.7063838223632039\n",
      "Number of time predicted fit  correct : 8005\n",
      "Number of time predicted fit  Wrong : 931\n",
      "Number of time predicted different than actual fit   : 3077\n",
      "RECALL  fit  is : 0.722342537448114\n",
      "TNR  fit  is : 0.7701234567901235\n",
      "PERCISION  fit  is : 0.8958146821844225\n",
      "FPR  fit  is : 0.22987654320987655\n",
      "BER  fit  is : 0.2537670028808813\n",
      "Number of time predicted large  correct : 1285\n",
      "Number of time predicted large  Wrong : 1681\n",
      "Number of time predicted different than actual large   : 687\n",
      "RECALL  large  is : 0.6516227180527383\n",
      "TNR  large  is : 0.8722644376899696\n",
      "PERCISION  large  is : 0.4332434254888739\n",
      "FPR  large  is : 0.1277355623100304\n",
      "BER  large  is : 0.23805642212864603\n",
      "Number of time predicted small  correct : 1399\n",
      "Number of time predicted small  Wrong : 1831\n",
      "Number of time predicted different than actual small   : 679\n",
      "RECALL  small  is : 0.6732435033686237\n",
      "TNR  small  is : 0.8597364792400797\n",
      "PERCISION  small  is : 0.43312693498452015\n",
      "FPR  small  is : 0.14026352075992032\n",
      "BER  small  is : 0.23351000869564834\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  1 0.7085646312450437\n",
      "Number of time predicted fit  correct : 8051\n",
      "Number of time predicted fit  Wrong : 950\n",
      "Number of time predicted different than actual fit   : 3031\n",
      "RECALL  fit  is : 0.7264934127413825\n",
      "TNR  fit  is : 0.7654320987654321\n",
      "PERCISION  fit  is : 0.8944561715364959\n",
      "FPR  fit  is : 0.2345679012345679\n",
      "BER  fit  is : 0.25403724424659274\n",
      "Number of time predicted large  correct : 1277\n",
      "Number of time predicted large  Wrong : 1682\n",
      "Number of time predicted different than actual large   : 695\n",
      "RECALL  large  is : 0.6475659229208925\n",
      "TNR  large  is : 0.8721884498480243\n",
      "PERCISION  large  is : 0.431564717810071\n",
      "FPR  large  is : 0.12781155015197568\n",
      "BER  large  is : 0.2401228136155416\n",
      "Number of time predicted small  correct : 1394\n",
      "Number of time predicted small  Wrong : 1778\n",
      "Number of time predicted different than actual small   : 684\n",
      "RECALL  small  is : 0.670837343599615\n",
      "TNR  small  is : 0.8637965374597825\n",
      "PERCISION  small  is : 0.4394703656998739\n",
      "FPR  small  is : 0.13620346254021756\n",
      "BER  small  is : 0.23268305947030127\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  2 0.7066481628337299\n",
      "Number of time predicted fit  correct : 8025\n",
      "Number of time predicted fit  Wrong : 942\n",
      "Number of time predicted different than actual fit   : 3057\n",
      "RECALL  fit  is : 0.7241472658364916\n",
      "TNR  fit  is : 0.7674074074074074\n",
      "PERCISION  fit  is : 0.8949481431917029\n",
      "FPR  fit  is : 0.2325925925925926\n",
      "BER  fit  is : 0.2542226633780505\n",
      "Number of time predicted large  correct : 1273\n",
      "Number of time predicted large  Wrong : 1686\n",
      "Number of time predicted different than actual large   : 699\n",
      "RECALL  large  is : 0.6455375253549696\n",
      "TNR  large  is : 0.8718844984802432\n",
      "PERCISION  large  is : 0.4302129097668131\n",
      "FPR  large  is : 0.12811550151975684\n",
      "BER  large  is : 0.24128898808239363\n",
      "Number of time predicted small  correct : 1395\n",
      "Number of time predicted small  Wrong : 1811\n",
      "Number of time predicted different than actual small   : 683\n",
      "RECALL  small  is : 0.6713185755534168\n",
      "TNR  small  is : 0.8612685766814769\n",
      "PERCISION  small  is : 0.4351216469120399\n",
      "FPR  small  is : 0.13873142331852306\n",
      "BER  small  is : 0.23370642388255317\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  5 0.7062516521279408\n",
      "Number of time predicted fit  correct : 8017\n",
      "Number of time predicted fit  Wrong : 942\n",
      "Number of time predicted different than actual fit   : 3065\n",
      "RECALL  fit  is : 0.7234253744811406\n",
      "TNR  fit  is : 0.7674074074074074\n",
      "PERCISION  fit  is : 0.8948543364214756\n",
      "FPR  fit  is : 0.2325925925925926\n",
      "BER  fit  is : 0.254583609055726\n",
      "Number of time predicted large  correct : 1273\n",
      "Number of time predicted large  Wrong : 1689\n",
      "Number of time predicted different than actual large   : 699\n",
      "RECALL  large  is : 0.6455375253549696\n",
      "TNR  large  is : 0.8716565349544073\n",
      "PERCISION  large  is : 0.4297771775827144\n",
      "FPR  large  is : 0.12834346504559271\n",
      "BER  large  is : 0.24140296984531157\n",
      "Number of time predicted small  correct : 1397\n",
      "Number of time predicted small  Wrong : 1814\n",
      "Number of time predicted different than actual small   : 681\n",
      "RECALL  small  is : 0.6722810394610202\n",
      "TNR  small  is : 0.8610387620652673\n",
      "PERCISION  small  is : 0.43506695733416384\n",
      "FPR  small  is : 0.13896123793473264\n",
      "BER  small  is : 0.23334009923685622\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words,rating and category for C=  10 0.7047316944224161\n",
      "Number of time predicted fit  correct : 7978\n",
      "Number of time predicted fit  Wrong : 928\n",
      "Number of time predicted different than actual fit   : 3104\n",
      "RECALL  fit  is : 0.7199061541238043\n",
      "TNR  fit  is : 0.7708641975308642\n",
      "PERCISION  fit  is : 0.8958005838760387\n",
      "FPR  fit  is : 0.2291358024691358\n",
      "BER  fit  is : 0.25461482417266573\n",
      "Number of time predicted large  correct : 1284\n",
      "Number of time predicted large  Wrong : 1704\n",
      "Number of time predicted different than actual large   : 688\n",
      "RECALL  large  is : 0.6511156186612576\n",
      "TNR  large  is : 0.870516717325228\n",
      "PERCISION  large  is : 0.42971887550200805\n",
      "FPR  large  is : 0.12948328267477205\n",
      "BER  large  is : 0.23918383200675722\n",
      "Number of time predicted small  correct : 1402\n",
      "Number of time predicted small  Wrong : 1836\n",
      "Number of time predicted different than actual small   : 676\n",
      "RECALL  small  is : 0.6746871992300288\n",
      "TNR  small  is : 0.8593534548797304\n",
      "PERCISION  small  is : 0.4329833230389129\n",
      "FPR  small  is : 0.14064654512026964\n",
      "BER  small  is : 0.23297967294512037\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for C in [0.01,0.1,1,2,5,10]:\n",
    "    modC = linear_model.LogisticRegression(C=C,class_weight='balanced')\n",
    "    modC.fit(Xtrain, Y_train)\n",
    "    pred_validC= modC.predict(Xtest)\n",
    "    correctC = pred_validC==Y_test\n",
    "    #BER(pred_valid,Y_test)\n",
    "    print(\"accuracy feature:bag of words,rating and category for C= \",C, sum(correctC)/len(correctC))\n",
    "    BER(pred_validC,Y_test,'fit')\n",
    "    BER(pred_validC,Y_test,'large')\n",
    "    BER(pred_validC,Y_test,'small')\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy feature:bag of words 0.8018107322231034\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "pred_valid= mod1.predict(Xtest)\n",
    "correct2 = pred_valid==Y_test\n",
    "print(\"accuracy feature:bag of words\", sum(correct2)/len(correct2))\n",
    "#print(len(correct1))\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'fit': 12860, 'large': 1125, 'small': 1147}) defaultdict(<class 'int'>, {'large': 1972, 'fit': 11082, 'small': 2078})\n"
     ]
    }
   ],
   "source": [
    "dic1= defaultdict(int)\n",
    "dic2=defaultdict(int)\n",
    "\n",
    "#print(l[0])\n",
    "for i in pred_valid:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic1[j]+=1\n",
    "for i in Y_test:                                       #num of fit/small/large\n",
    "    j=str(i)\n",
    "    dic2[j]+=1\n",
    "    \n",
    "print(dic1,dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1= defaultdict(int)\n",
    "dic2=defaultdict(int)\n",
    "dic3=defaultdict(int)\n",
    "for i,j in zip(pred_valid,Y_test):\n",
    "    if(i==j and i =='small'):\n",
    "        dic1[i]+=1\n",
    "    elif (i==j and i =='large'):\n",
    "        dic1[i]+=1\n",
    "    elif(i==j and i =='fit'):\n",
    "        dic1[i]+=1\n",
    "    elif(i=='small'):\n",
    "        dic1['predict small but wrong']+=1\n",
    "    elif(i=='large'):\n",
    "        dic1['predicted large but wrong']+=1\n",
    "    else:\n",
    "        dic1['predict fit but wrong']+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'predict fit but wrong': 2315, 'fit': 10545, 'large': 804, 'small': 784, 'predicted large but wrong': 321, 'predict small but wrong': 363}) defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "print(dic1,dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "12\n",
      "32\n",
      "39\n",
      "45\n",
      "57\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in data1:\n",
    "    if int(i['size']) > j:\n",
    "        print(i['size'])\n",
    "        j=int(i['size'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
